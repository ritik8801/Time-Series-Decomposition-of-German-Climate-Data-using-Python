{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4959710,"sourceType":"datasetVersion","datasetId":2848117}],"dockerImageVersionId":30369,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Download and transformation of German temperature data (1990-2021).","metadata":{}},{"cell_type":"markdown","source":"## On this notebook\nThis notebook describes the download and transformation of historical weather data from Germany. The source of the data is the Climate Data Center (CDC) of Deutscher Wetterdienst (DWD, engl. German Meteorological Service). For more information about this data source, see [this README about the CDC-OpenData area](https://opendata.dwd.de/climate_environment/CDC/Readme_intro_CDC_ftp.pdf).\n\nFrom several available data sets, historical data with a 10 minute frequency have been choosen. Data start from 1990-01-01 and last until 2021-12-31. However, the coverage of this date range varies strongly between the weather stations contained in the data set.\n\nThe download (executed at 2023-01-17) includes 1611 single files, each file containing a time series with 10 minutes frequency for at least one weather station. The data are stored in \"long format\", resulting in > 500 million rows. As my interest was the temperature values, I converted the \"long format\" to \"wide format\", discarding all numerical values except the temperature data and setting the weather stations as columns. The result file contains a time series with temperature data from 513 weather stations. The script can be used to extract the other numerical values (available values see documentation below) with the same sequence of transforming steps.\n\nThe transformation faces several problems due to the large number of data points:\n* On Google colab, the memory consumption (currently 12 GB for free users) is exceeded.\n* On Kaggle, the disk space in the working directory (for free users) is exceeded.\n* On my local machine (with 16 GB RAM), the process is running properly, but needs several hours to complete.\n\nThe transformation is performed in the following steps to avoid exceeding the memory limit on my machine:\n* Download of the original files.\n* Read the files, extract the numerical column and write a bunch of intermediate bigger files.\n* Finally merge this bigger file into one big file.\n* To drop duplicates, isolate the data for each weather station.\n* Unstack the weather stations from long to wide format.\n* Final cleanup and export:\n    * replace the original spaceholders for missing values (-999.0) by nan values\n    * sort the columns by ascending weather station id\n    * fill the gaps in the time series, making it possible to create a time series with 10 minutes frequency\n\nIt is recommended to execute the cells step by step and monitor the process. **The script is not optimized or tested for automation.**\n\nI might add a version of the script that works without writing temporary files on Kaggle, because the bigger memory limit on Kaggle (currently 30 GB) might allow the extraction without writing temporary files.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport bs4\nimport requests\nimport glob\nimport shutil","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set some paths\nAs the processing of the data requires quite some amount of memory, it will be separated in several steps of writing temporary output files. In the following cell, some paths are defined for this process:\n* **download_path**: directory for the original download files\n* **extraction_path**: directory for the extracted downloaded files, in case you want to extract before parsing in\n* **tmp_path**: directory for writing intermediate files\n* **result_path**: directory for the result file\n\n**Caution**: Make sure these directories exist before executing the cells that make use of the directories.","metadata":{}},{"cell_type":"code","source":"# we use the current directory for writing the downloaded files\ndownload_path   = \"download/\"\nextraction_path = \"extracted/\"\ntmp_path        = \"tmp/\"\nresult_path     = \"results/\"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download of the original files\nIn case you want to download the files using this notebook, you can use the following two cells.","metadata":{}},{"cell_type":"code","source":"def download_files(url, output_path):\n    r = requests.get(url)\n    data = bs4.BeautifulSoup(r.text, \"html.parser\")\n    for l in data.find_all(\"a\"):\n        filename = l['href']\n        if filename != \"../\":\n            print(filename)\n            r = requests.get(url + filename)\n            open(output_path + filename, 'wb').write(r.content)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we download the historical data\nurl = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/air_temperature/historical/\"\n\n# execute the downloads\ndownload_files(url, download_path)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read in the original files\n* read the original files\n* extract the relevant information:\n    * measurement date\n    * weather station id\n    * one of the numerical values\n\nThe following numerical values are available:\n* **QN**: quality level of next columns coding see paragraph \"Quality information\"\n* **PP_10**: pressure at station height hPa\n* **TT_10**: air temperature at 2m height °C\n* **TM5_10**: air temperature at 5cm height °C\n* **RF_10**: relative humidity at 2m height %\n* **TD_10** dew point temperature at 2m height °C\n\nIn the following example code, we extract a matrix with the measurement date time series as index and the weather station ids as columns. We use the \"air temperature at 2m height °C\" (**TT_10**) as the numerical value.","metadata":{}},{"cell_type":"code","source":"# list of downloaded original data files\nfilenames = glob.glob(extraction_path + \"*.txt\")\nnumber_of_files = len(filenames)\n\n# columns of interest in the original data files\ntarget_index_col = 'MESS_DATUM'\ntarget_header_col = 'STATIONS_ID'","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set the target numeric value for the rest of the notebook\nIn case you want to extract the same file format for other information, you can change target value here (possible target values see description above).","metadata":{}},{"cell_type":"code","source":"target_numeric_col = 'TT_10'\nusecols = [target_index_col, target_header_col, target_numeric_col]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remarks on the data types used for import\n* The measurement date (MESS_DATUM) will be imported as string. The reason is that the data contain some duplicates (based on the combination MESS_DATUM/STATIONS_ID, and the pandas built in functions for duplicate detection (`duplicated()` or `drop_duplicates()`) do not work properly with the datetime values.\n* Although it would be desirable to use `float16` as import type for the numerical data (to save memory), this does not work properly, as the imported data contain rounding errors when using `float16`. Using just `float` results in `float64` values, which consume quite a lot of memory. However, there seems no way around this.\n","metadata":{}},{"cell_type":"code","source":"# data types for import\ndtypes_dict = {'MESS_DATUM' : 'str',\n               'STATIONS_ID' : 'int16',\n               'QN' : 'int8',\n               'PP_10' : 'float',\n               'TT_10': 'float',\n               'TM5_10' : 'float',\n               'RF_10' : 'float',\n               'TD_10' : 'float'}","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the number of original files for preprocessing the original data in one step and saving to intermediate files.\n# Lower the stepsize to reduce memory consumption if necessary.\nstepsize = 20\n\n# create the bundles with the configured stepsize\nstart_list = list(range(0, (number_of_files - stepsize), stepsize))\nstop_list = list(range(stepsize, number_of_files, stepsize))\nstart_stop_list = list(zip(start_list, stop_list))\n\n# add the \"rest\"\nstart_stop_list.append((stop_list[-1], number_of_files))\nprint(start_stop_list)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor start, stop in start_stop_list:\n    count += 1\n\n    if count % 10 == 0:\n        print(\"Reading original files \" + str(start) + \" to \" + str(stop))\n\n    dfs = []\n    for i in range(start, stop):\n        # several options to optimize memory behaviour:\n        # - import only the cols needed\n        # - put MESS_DATUM and STATIONS_ID in the index\n        #   (if you have enough memory, leaving this out will be faster)\n        # - use minimal datatypes\n        df = pd.read_csv(filenames[i],\n                         usecols=usecols,\n                         index_col=[target_index_col, target_header_col],\n                         dtype=dtypes_dict,\n                         sep=\";\")\n\n        if df.shape[0] > 0:\n            dfs.append(df)\n\n    df = pd.concat(dfs, ignore_index=False)\n    df.to_csv(tmp_path + \"tmp_\" + str(start+1) + \"_\" + str(stop) + \".csv\", index=True)\n\nprint(\"Writing of intermediate files finished.\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert temporary files to one file per weather station.\nThe data for one weather station (STATIONS_ID) can be delivered in several of the original files (for different measurement time periods).\n\nWe therefore now merge the intermediate files to one file and then extract one file per weather station (STATIONS_ID). This is now less memory consuming, because we have reduced the data to only one of the contained numerical values.","metadata":{}},{"cell_type":"code","source":"def combine_files(tmp_files, tmp_all):\n    with open(tmp_all, 'wb') as outfile:\n        for i, filename in enumerate(tmp_files):\n            print(filename)\n            if filename == tmp_all:\n                continue\n            with open(filename, 'rb') as readfile:\n                if i != 0:\n                    readfile.readline()\n                shutil.copyfileobj(readfile, outfile)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We merge all outputfiles into one.\ndel(df)\ntmp_all = tmp_path + 'tmp_all.csv'\ntmp_files = glob.glob(tmp_path + '*.{}'.format('csv'))\ncombine_files(tmp_files, tmp_all)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drop duplicates per weather station\nDuplicates only occur for single weather stations, because a duplicate is a row with identical values in the columns MESS_DATUM and STATIONS_ID.\n\nDuplicate search over the complete dataframe is very inefficient. So we search and delete duplicates per weather station.\n\nThe duplicate free data are stored in intermediate files \"station_<STATIONS_ID>.csv\" in the tmp directory.","metadata":{}},{"cell_type":"code","source":"# we repeat this in case we resume excecution\ntmp_all = tmp_path + 'tmp_all.csv'\n\ndf = pd.read_csv(tmp_all,\n    index_col='STATIONS_ID',\n    dtype=dtypes_dict,\n    sep=\",\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_deleted_duplicates = 0\nfor station in df.index.unique():\n    # select and count before cleaning\n    tmp = df.loc[station, :].copy()\n    before = len(tmp)\n\n    # execute removal of duplicates\n    tmp.reset_index(inplace=True)\n    tmp.drop_duplicates(subset=['MESS_DATUM', 'STATIONS_ID'], keep='first', inplace=True)\n\n    # do the statistics\n    after = len(tmp)\n    deleted = before - after\n    count_deleted_duplicates += deleted\n\n    # write result\n    print(\"Cleaning STATIONS_ID \" + str(station) + \": \" + str(deleted) + \" deleted duplicates.\")\n    tmp.to_csv(tmp_path + \"station_\" + str(station) + \".csv\", index=False)\n\nprint(\"Deleted duplicates: \" + str(count_deleted_duplicates))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In the above output, we recognize that the weather station 3023\n# has extraordinarily many duplicates. Let's check this.\n\ntmp = df.loc[3023, :].copy()\ntmp.reset_index(inplace=True)\nduplicates = tmp[tmp.duplicated(subset=['MESS_DATUM', 'STATIONS_ID'], keep=False)].copy()\nduplicates.sort_values(by=['MESS_DATUM', 'STATIONS_ID'], inplace=True)\nduplicates.head(100)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We see indeed a lot of correctly classified duplicates.\n# Let's check another example.\n\ntmp = df.loc[3348, :].copy()\ntmp.reset_index(inplace=True)\nduplicates = tmp[tmp.duplicated(subset=['MESS_DATUM', 'STATIONS_ID'], keep=False)].copy()\nduplicates.sort_values(by=['MESS_DATUM', 'STATIONS_ID'], inplace=True)\nduplicates.head(100)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ok, we can be quite confident that the duplicate detection worked well.\n# So let's clean up.\ndel(df)\ndel(tmp)\ndel(duplicates)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unstacking the STATIONS_ID\nWe use `int64` as type for the MESS_DATUM column here, because the unstacking runs much faster with this type. ","metadata":{}},{"cell_type":"code","source":"unstacking_dtypes_dict = {'MESS_DATUM': 'int64', 'STATIONS_ID': 'short', target_numeric_col: 'float'}\nstation_filenames = glob.glob(tmp_path + \"station_*.csv\")\ncount_rows = 0\ndfs = []\nfor station_file in station_filenames:\n    print(station_file)\n    df = pd.read_csv(station_file,\n                     index_col=['MESS_DATUM', 'STATIONS_ID'],\n                     dtype=unstacking_dtypes_dict,\n                     sep=\",\")\n\n    # We count the rows of the imported data to compare the number\n    # to some output from above, just to double check that we lost no data\n    count_rows += len(df)\n    df = df.unstack()\n    dfs.append(df)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From the output a view cells above, we can see that we had originally 509311968 entries,\n# from which we deleted 2238399 duplicates. So we expect to see a total number of\n# 509311968 - 2238399 = 507073569 rows in the station_*.csv files.\n\n# We also keep this number in mind for later comparison with the concatenated unstacked version.\ncount_rows","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat(dfs, axis=1)\ndel(dfs)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data for the final export","metadata":{}},{"cell_type":"code","source":"# Dropping metalevel of index resulting from unstacking.\ndf = df.droplevel(0, axis=1)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So let's compare ...\ndf.count().sum()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We left this step for now to be able to compare the number of\n# transformed entries with the original numbers.\ndf.replace(-999.0, np.nan, inplace=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's remember the number of values without the -999.0 data points\ndf.count().sum()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert columns in int type and sort them ascending from left to right\ncolumns = []\nfor col in df.columns:\n    columns.append(int(col))\n\ndf.columns = columns\ndf = df[sorted(df.columns)]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert index to datetime index and sort it\ndf.index = pd.to_datetime(df.index, format='%Y%m%d%H%M')\ndf.sort_index(inplace=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill the gaps in the time series\ncomplete_range = pd.date_range(df.index.min(), df.index.max(), freq=\"10min\")\ncomplete_df = pd.DataFrame(index=complete_range)\nprint(\"Length of df       range: \" + str(len(df.index)))\nprint(\"Length of complete range: \" + str(len(complete_df.index)))\n\n# Merge and check if the range has the correct length afterwards.\ncomplete_df = complete_df.join(df)\ncomplete_df.index.name = \"MESS_DATUM\"\nprint(\"Length of merged   range: \" + str(len(complete_df.index)))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Are the data still unchanged?\ndf.count().sum()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_file = result_path + \"/\" + target_numeric_col + \".csv\"\ndf.to_csv(result_file, index=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(df)\ndel(complete_df)\ndel(complete_range)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's check the extracted data","metadata":{}},{"cell_type":"code","source":"# we convert to datetime in a separate step, because we can provide the format, which is more efficient\ndf = pd.read_csv(result_file, index_col=\"MESS_DATUM\")\ndf.index = pd.to_datetime(df.index, format=\"%Y-%m-%d %H:%M:%S\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have now a reduced value count due to the replacement of -999.0 values by nan values.\ndf.count().sum()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(df, )\nplt.title(\"Proportion of non-null data per weather station in complete date/time range\", fontdict={'size' : '20'});","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's view some of the weather station data","metadata":{}},{"cell_type":"code","source":"def plot_weather_station_data(df, stations_id):\n    one_day_window = 10*24\n\n    fig, ax = plt.subplots(figsize=(20, 12))\n    df.loc[:, str(stations_id)].plot()\n    df.loc[:, str(stations_id)].rolling(one_day_window).mean().plot()\n\n    plt.title(\"STATIONS_ID == \" + str(stations_id), fontdict={\"size\" : 20}, pad=20)\n    plt.xlabel(\"Years\", fontdict={\"size\" : 20})\n    plt.ylabel(\"Temerature in °C\", fontdict={\"size\" : 20})\n    plt.legend([\"All values in 10 minute frequency\", \"Daily mean values\"], prop={'size': 14})\n\n    plt.show()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_weather_station_data(df, 3)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_weather_station_data(df, 298)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_weather_station_data(df, 1239)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_weather_station_data(df, 19172)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}